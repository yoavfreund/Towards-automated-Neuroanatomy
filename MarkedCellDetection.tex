\documentclass[11pt]{article}
%\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{arydshln}
\usepackage{ulem}
\usepackage[table,xcdraw]{xcolor}

\usepackage{wrapfig}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}


\setlength{\parskip}{0cm}
\setlength{\parindent}{1em} 

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\newcommand{\reals}{\mathbb{R}}

\newcommand{\comment}[3]{{\color{#1} {\bf #2 :} #3}}
%\newcommand{\comment}[3]{}  % suppress comments
% Use these macros to make comments.
\newcommand{\kui}[1]{\comment{blue}{Kui}{#1}}
\newcommand{\yoav}[1]{\comment{purple}{Yoav}{#1}}
\renewcommand{\beth}[1]{\comment{red}{Beth}{#1}}
\newcommand{\david}[1]{\comment{cyan}{David}{#1}}
\newcommand{\zhongkai}[1]{\comment{brown}{Zhongkai}{#1}}

\renewcommand\floatpagefraction{.99}
\renewcommand\topfraction{.99}
\renewcommand\bottomfraction{.99}
\renewcommand\textfraction{.1}

\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}

\title{Towards computer assisted Neuroanatomy}
\author{Kui Qian, Zhongkai Wu, Beth Friedman, David Kleinfeld, Yoav Freund}

\begin{document}

\maketitle

\begin{abstract}
  An important and time consuming task in neuroanatomy is the
  annotation of marked cells. Modern XXX techniques allow marking
  specific cell groups with fluorescent dyes. Modern high throughput
  scanning microscope allow high resolution multi-channel imaging of
  the sectioned brain. However, manual identification of marked cells
  is prohibitively time consuming.  We present a machine
  learning methodology for developing digital assistants that
  significantly reduce the labor of the anatomist while improving the
  consistency of the annotation.

  Our methodology is based on designing, for each annotation problem,
  a large number of features and using boosting on manually annotated
  images to select the most predictive features. We describe two
  annotation tasks to which we applied this approach. The first, more
  complex task is detecting specific structures in the brainstem of
  the mouse. The second is the detection of individual cells marked by
  a fluorescent indicator. We provide the details of both of these
  applications and show that they significantly reduce the anatomist's
  labor in completing these tasks.
  
  %We translate each cell image into a feature vector that
  %includes aspect ratio, orientation and area, as well as additional
  %features derived using a graph Laplacian. The algorithm uses thehttps://www.overleaf.com/project/626c35fc11c79a37dffd2ffc
  %statistical distribution of these features vectors to identify brain
  %structures.
\end{abstract}

\section{Main}

We propose an adaptive system for assisting neuroanatomists in the
task of labeling marked cells. Rather than classifying all cell
candidates as either positive or negative we allow the classifier to
output ``unsure''. We demonstrate that this system can reduce the labor of the anatomist by a factor of XXX without reducing accuracy.


\subsection {Significance for Neuroscience}

How do we study the computational power of brains? Brains are composed
of circuits at multiple scales of organization, the
microscopic-scale of connections between neurons, i.e., one to ten
micrometers, the mesoscopic-scale of collections of neurons with
similar functionality, i.e., hundred of micrometers to
millimeters. Molecular tagging of neuronal cell types by the
expression of genetically encoded reporters and light-level imaging of
the cells and tags, using both transmission and fluorescent
microscopy, plays an essential role in this process. The resulting raw
data files are extremely large, from 1 to 100 TB per rodent brain. The
current means for quantification and spatial mapping of tagged
neuronal populations in brain sections require labor and time intensive
manual annotation by expert neuroanatomists. How can machine learning assist
with this process and minimize the amount of manual work involved
while maintaining accuracy and consistency?

We use mouse brains that are serially sectioned and counterstained
with one fluorescent tag to reveal all brain cells.  In addition,
specific cell types are labeled with a second, different fluorescent
tag. This results in a dual tagging approach where all cells are
labeled by an inexpensive counterstain and a subset of interest is
labeled by a second experimentally incorporated fluorophore. This has
the advantage of narrowing the population of false detections as
labeled neurons undergo a two-factor authentication scheme

\yoav{What is a two factor authentication scheme, sounds like
  something used to log into a secure web site....}

% Further, how can machine learning assist neuroscientists is to combine experimental results from the brains of different animals.

%Neuroanatomical observations depend on mapping data from images, obtained as consecutive planar sections, to reference brains.
%This presents a number of challenges. In many cases the individual sections may carry distortions, and further need to be aligned to form a three-dimensional representation of a brain. The second challenge is the identification of brain structures. Classically, this relies on the use of a stain, i.e., a labels that marks a restricted part of the brain cells and allows both individual cells and clusters of like cells to be identified. This is an art form
% \yoav{ Instead of "art form" I would say "requires significant expertise and many hours of work". I would also say something about the need to create work-flows that combine less expert annotators (Julian and Marissa) and more expert annotators (Beth) to achieve the required throughput with sufficient accuracy.} , as different stains highlight different organelles of brain cells and different sectioning techniques reveal yield seemhttps://www.overleaf.com/project/626c35fc11c79a37dffd2ffcingly different textures in the organization of neurons. Currently, human scanning is used to find the best match of a given structure, from the subject brain, to an annotated reference brain atlas. This match to sample approach does not readily support assessment of observational differences between laboratories nor provide a means for assessing and compensating for common systematic errors.
% Critically, human observers do not provide a path toward either understanding of critical features that define a structure nor a means toward automation.
% \yoav{ I don't understand the last sentence. In my mind, it is a question of resources. My assumption is that the "ground truth" is achievable by an expert anatomist spending sufficient time. What we are attempting to do is (1) save anatomist time (2) allow less expert anatomist to do the task (3) provide meaningful confidence levels and (4) provide explanations for the confident predictions.}

\subsection{Significance to machine learning}
The standard goal of a machine learning goal is to reach a performance
level close to or better than a human. Thereby replacing the human.
However, in practice, it is hard to compare the performance of a
learned model to that of a human. That is because different humans
often disagree with each other on a significant fraction of the
labels. This phenomenon, called inter-rater disagreement
rate~\cite{general} has been studied in clinical neuroscience~\cite{},
less in neuroscience research. One contribution of this paper is an
evaluation if inter-rater agreements in the context of marked cell
detection.

Inter-rater disagreement rates quantify the average level of
disagreement. However, the level of disagreement on each particular
example can differ. We say that an example is ``easy positive'' if
most graders label it positive and ``easy negative'' similarly for
negative. We say that an example is ``hard'' is significant
disagreement between labelers.

We allow our learned classifiers to output ``sure positive'',
``unsure'', and ``sure negative''. The goal is to achieve high accuracy and
on the confident classifications while minimizing the fraction of unsure examples.

To Be Completed.

\subsection{Comparison to other work}
Machine learning methods, mostly based on Neural Network (NN), have
been used to automate brain section analysis~\cite{}. However, as NN
models are black boxes that define input-output relationships, the
models provide no insight as to how decision are made. Nor do NNs give
a measure of confidence. It is thus very hard for a neuroanatomist to
understand {\em why} the NN made particular decisions. In this paper
we present an alternative approach with models that operate in a way
more similar to the neuroanatomist and that generates outputs which
the neuroanatomist can interpret and correct.


% Manual neuroanatomical analysis of brain sections is critical for creating accurate quantitative results. On the other hand, performing neuroanatomical analysis is expensive as it requires many hours of work of an expert anatomist.

% We consider two labor intensive tasks. Localizing anatomical structure and counting marked cells. Both tasks involve identifying and marking locations in images and both require expertise.


\iffalse
\subsubsection{Allowing the computer to say ``I don't know''}

When applying machine learning methods to real world problems a common 
expectation is that, with a sufficiently large training set, the
machine would have higher accuracy than a human expert. To measure
accuracy we need access to the so-called {\em ground truth}. In the
context of neuroanatomy it is sometimes possible to use a special
stain to evaluate the ground truth. However, in many cases the ground
truth is based on human judgement, and humans sometimes disagree. When
humans assign different labels to an example we cannot say whether the
computer prediction is correct or not. If the disagreement between
human raters is significant, it becomes hard to claim that the
computer is more accurate than the human.

\cite{Conformal} offers a way out. To the two labels of a binary prediction
$0,1$ we add a third label called ``I don't know'' indicated by
``?''. When evaluating the prediction we consider two quantities: the
fraction of times that the prediction is ``?'' which we call the
abstention rate, and the fraction of the non-abstentions that are
wrong, which we call the error rate.


The level of agreement between human labelers is studied in
inter-rater and intra-rater experiments~\cite{}. Inter-rater agreement
measures the rate of agreement between different human
labelers. Intra-rater agreement measures the rate of agreement between
labels chosen by the same labeler at different times.  A common way to
quantify the of the agreement between two raters is the Cohen's
$\kappa$ (kappa) coefficient~\cite{mchugh2012interrater}~\footnote{ ,
  $\kappa$ is computed from two more basic quantities: $0\leq a\leq 1$ is
  the fraction of cases on which the two raters agree, and
  $0\leq c\leq 1$ is the fraction of agreements that would occur by
  chance if the two raters are statistically independent.
  The definition of kappa is $\kappa=\frac{a-c}{1-c}$.

  If $\kappa=1$, The raters always agree, if $\kappa=0$ the rate of
  agreement corresponds to chance, and if $\kappa<0$ then the rate of
  agreement is lower than chance, i.e. the two raters tend to have
  different opinion. An interpretation of $\kappa$ recommended by
  Cohen \cite{mchugh2012interrater} is: $\kappa\leq 0$: no agreement,
  $0< \kappa\leq 0.20$: none to slight agreement,
  $0.2<\kappa\leq 0.40$: fair agreement, $0.4<\kappa\leq 0.60$:
  moderate agreement, $0.6<\kappa\leq 0.80$: substantial agreement,
  and $0.8<\kappa\leq 1.00$: perfect agreement.}

Some values of $\kappa$ quoted in the inter-rater agreement literature
for tasks similar to ours include.....

  An important observation on locating both structures and marked cells
is that while a typical section will contain some hard to identify
features and locations, most of the identifications are relatively
easy. This observation supports our approach which uses the computer as
to assist, rather than replace the human anatomist. We
use {\em confidence rated detectors}. These detectors
associate with each detection a confidence score. High confidence
detections are labeled by the computer while low condidence detections
are passed on to the human anatomist.
\fi

\subsubsection{Quantifying work reduction}
We compare the amount of work of the anatomist when unaided to the
amount of work when aided by confidence rated detections.
When using an accurate and confident detection system the work of the
anatomist reduces to the following components
\begin{enumerate}
\item {\bf Performing Quality assurance on confident detections} In this step the
  anatomist recieves a small sample of the confident detections and
  verifies that they are correct. The sample size depends on the
  desired accuracy.
\item {\bf searching for misses:} In this step the anatomist looks
  for locations that were completely missed by te detector.
\item {\bf classifiying the unconfident detections} In this step
  the anatomist labels {\em all} of the low confident predictions.
  \end{enumerate}
Steps 1 and 2 are based on samples and are therefor relatively light.
Most of the work of the anatomist is in step 3. We call the ratio
between the unconfident detection and the confident detections the 
``effective work ratio''. When the effective work ration is small the
savings in manual work is large.

\iffalse
\subsubsection{Assistive AI} Traditionally, the goal of artificial
  intelligence is to design algorithms which mimic human behavior. AI
  systems are expected to perform as well as humans and, eventually,
  better than humans.

Indeed, in some domains, such as the game of go \cite{silver2017mastering} deep neural
networks (DNN)  have achieved super-human capabilities. In addition,
on highly curated image classification datasets such as ImageNet~\cite{deng2009imagenet} and
CIFAR~\cite{krizhevsky2009learning} DNNs give performance that, some argue, is better than
human. 

However, DNNs have some fundamental drawbacks that limit their
applicability to real-world problems. One problem is that DNNs
require large amounts of training data.
In some setups, such as alpha-go and the labels are generated automatically as
the program plays against itself. However often labels are generated by a
human expert and collecting large amounts of data is
prohibitively expensive. This is the case in the context of this
paper, where each labeled example corresponds to the 3D location of a
cell or a structure.

A second problem is that the training data and the test data are assumed to
come from the same distribution. However, in most real-world
applications the test data is drawn after the training is complete and
is often generated by a setup that is different from the one used to
collect the training data.
Specifically, in brain imaging, two brain section images from the same
location in different brains are likely to look quite differently.
These images vary for a variety of reasons including: animal to animal variability,
variability in the preparation process, including staining,
sectioning and imaging. In order to create robust classifiers, that do
not need to be retrained on each brain, we need a learning algorithm
that identifies robust patterns that are preserved across brains.
identifies patterns that are consistent across brains.

A third problem arises from variability in human labeling. We use
the term {\em inter raters variation} to refer to the difference in
labels between anatomists. We use the term {\em intra rater
  variation} to refer to the difference in the labels generated by the
same anatomist at different times. Inter and intra rater variations
have been studied in medicine~\cite{gellhorn2013inter}. One
contribution of this paper is an evaluation of rater variation in
neuroanatomy and it's relation to the {\em confident/un-confident}
classifications generated by our methods.
\fi

\iffalse
\subsubsection{Cell based analysis}

In~\cite{chen2019active} Chen et al used a sliding window and a
Neural network to detect brain structures. The window size is fixed at
$100 \mu m \times 100 \mu m$ which means that a typical window contains 10-100 cells.
The window is used as input to a pre-trained neural network which maps
each window to a vector of 1,000 features. The output layer of
this network is trained using logistic regression. The earlier layers
are fixed. Chen et al used this system to demonstrate the first
structure detection that operates at the resolution of single cells.
However, the concept of a cell is not directly used by this as the
input to the neural network consists of raw pixel values, and not any
higher-order features.

On the other hand, anatomist base their analysis on cytoarchitecture,
which, in turn, is based on the shapes of individual cells and the
relationshp between them. This makes it harder for the anatomist to
understand the decisions of the detector.

To remedy this problem and make the detections explainable, we use
features that are based on the shapes of individual cells.
\fi
e
\section{Results}

\subsection{Detecting marked cells}
Detection of marked cells is a labor intensive activity. Our estimate
is that it takes a trained anatomist about 20 seconds to detect a
single cell. The total number of marked cells varies between 1000 and
20,000 cells, or 5-50 hours. As marking cells is tiring, typically no
no more than 2 hours per day, which means that manual detection can
take weeks. One way to reduce manual labor is to perform detections
on one out of 4 sections.

The system that we design uses a cell extraction routine, which
extracts, on the order of 100,000 ``cell-candidates''. We train
a classifier to divide these into three groups: {\it
sure/unsure/negative}. These are desiged so that the ``sure'' and the
``negative'' have accuracy comparable to that of humans and therefor
can be labeled automatically. The candidates labeled ``unsure'' are too
hard to call and are left for humans to label. Human labeling is
thereby reduced to the followiing three tasks:
\begin{enumerate}
\item Labeling a sample of the 250 ``sure'' and 250 ``unsure'' to
  verify the accuracy of the ``sure'' and estimate the accuracy of the
  unsure.
  \item Labeling five section in an {\em unaided} mode to estimate the
    false negatives of the detector.
  \item Labeling all of the ``unsure'' 
\end{enumerate}

In the experiments reported here we had two independent human labelers
perform tasks 1 and 2. This allows us to evaluate the rate of
disagreement between the labelers. The disagreement rate sets an upper
bound on the accuracy of our classifier as it quantifies the
reliability of our ground truth.

\begin{figure}[t]
  \includegraphics[width=\textwidth]{figures/Marked_cell_detections.png}
  \caption{{\small Data to illustrate automated detection of cells along with quality control. The same section is imaged in two channels. The left panel shows the Neurotrace counterstain and detector outputs (dots); the right panel images the GFP fluorescently labeled premotor neurons and detector outputs (dots). Dot size and color corresponds to machine-only vs human QC evaluation of the machine detections. Yellow dots represent confident machine-detections, large cyan dots represent unconfident detections. Small dots correspond to human feedback, white corresponds to positive feedback from the human (that is, the machine detection is correct), purple corresponds to negative feedback (incorrect machine detection). Finally premotor neurons that are missed by the two classes of machine detections (confident and unconfident) are also marked, as shown by the smallest yellow dots.}}
\label{fig:kmeans}
\end{figure}


\section{Methods}
\subsubsection{Boosting and sparse representations}
An important part of the design of any learning algorithm is finding a
representation of input feature vectors that captures the aspects that are
most relevant for the classification task. In some situations deep
neural networks can find internal representations autonomously,
without human intervension. However, a close look at the design of
alpha-Go~\cite{silver2017mastering} reveals the high level of human
expertise used to design the features used by the NN.

Boosting~\cite{FreundSc97,schapire2013boosting} is another popular
learning algorithm which combines a large number of so-called ``weak''
rules to construct a single ``strong'' rule. Here we follow an
approach to feature detection for boosting that can be described as
{\em the kitchen sink approach}. This approach starts by the human
constructing a very large number of candidate rules. The boosting
algorithm performs both {\em feature selection} i.e. finding the rules
that provide significant information about the label. As well as feature weighting and
combination, i.e. finding how to combine the informative features to
predict the label.\footnote{Popular boosting software, such as XGBoost and
  LightGBM, use decision trees to combine the features.}

In general, increasing the number of features or rules increases the
danger of over-fitting. However, as shown in ~\cite{SchapireFrBaLe98}, the
number of features has only a small influence on overfitting. rather,
it was shown, that the distribtuion of the large {\em normalized
  margin} guarantees low overfitting even if the number of features
goes to infinity.


\subsubsection{Boosting and object detection}
The original Adaboost algorithm is designed for the problem of binary
classification. On the other hand, the problems we attack in this
paper are those of {\em object detection} where the object to be
detected is a structure or a marked cell.

In~\cite{violajones01} Viola and Jones demonstrated that boosting
achieves superior speed and accuracy in the task of face detection,
which is now a universal feature in smart phones and cameras.  The
approach used is the pospular {\em sliding-window} approach used in
many other system.  The input to the classifier consists of windows of
different sizes and locations in the image. At the training phase each
window is labeled +1 if it contains the object and 0 if is does
not. At test time the sliding window is used to generate inputs for
the detector and windows that that recieve a high score are defined to
be the predictions.

Viola and jones used so-called {\em Haar features} to represent the
features. The set of features is large ($\sim 10^5$) and was designed
specifically for objects where horizontal and vertical contrasts are
dominant, based on the assumption that faces in photos are usually
upright. This is an example of the {\em kitchen sink} approach where
many features are proposed, but only few are used in the final detector.

We use a very similar design to that of Viola and Jones. We use a 2D
detector to identify marked cells and a 3D detector to identify the 3D
location of a structure in a sectioned brain.


\section{Discussion}

A challenge for extant methods is the
identification of brain structures. Classically, this relies on the
use of a stain, i.e., a labels that marks a restricted part of the
brain cells and allows both individual cells and clusters of like
cells to be identified. In many cases the individual sections may
carry distortions, and further need to be aligned to form a
three-dimensional representation of a brain. Currently, human scanning
is used to find the best match of a given structure, from the subject
brain, to an annotated reference brain atlas. This match-to-sample
approach does not readily support assessment of observational
differences between laboratories nor provide a means for assessing and
compensating for common systematic errors. An extended ML approach
would support a mechanism for ML “self-annotation” of structures in
sections. The output tag of a structure would be readily proof-read by
human observation. Importantly, the curated set of output tags could
serve as an objective set of fiducials. Fiducials are needed for
alignment to other atlases, such as the Allen Atlas but there is no
standardized method for fiducial selection.

This annotation allows tags cells to be placed into a common reference
space so that the spatial distributions of functionally identified
cells can be compared. This ML based tool supports detection of the 3d
centroids of brain structures in serially sectioned counterstained
mouse brains. The ML structure detectors essentially perform ML
self-annotation of brain structures which can be used as fiducials for
a more systematic registration of experimental brains into the widely
accepted common reference space, the Allen Atlas. ML supported
registration of brains to a common reference space, in turn allows
co-display of ML labeled neuron data from different experimental
brains.

A synergy of our ML cell and ML fiducial detection can support
combining data to common reference space. The ML detected labeled
neurons are in the same brain stack as the ML detected landmarks and
can be exported in a tethered fashion to new reference spaces. Thus
our new algorithms provide assistance for the persistent challenge of
combining connectomic data from different experimental brains into a
common reference space. This will allow 3d comparisons of connectomic
brain labeling data to help provide metrics for biological and for
technical variability that underpins these studies.

The central motivation of the work presented here is to develop tools
to leverage human expertise rather than attempt to replace it;
leveraging demands the use of ML methods that support meaningful
confidence levels and can provide explanations for these confident
predictions.

 \bibliographystyle{splncs04}
 \bibliography{Reference,bib}

\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
