\documentclass[runningheads]{llncs}
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{arydshln}
\usepackage{xcolor,ulem}

\usepackage{wrapfig}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\setlength{\parskip}{0cm}
\setlength{\parindent}{1em}

\newcommand{\reals}{\mathbb{R}}

\newcommand{\comment}[3]{{\color{#1} {\bf #2 :} #3}}
%\newcommand{\comment}[3]{}  % suppress comments
% Use these macros to make comments.
\newcommand{\kui}[1]{\comment{blue}{Kui}{#1}}
\newcommand{\yoav}[1]{\comment{purple}{Yoav}{#1}}
\renewcommand{\beth}[1]{\comment{red}{Beth}{#1}}
\newcommand{\david}[1]{\comment{cyan}{David}{#1}}
\newcommand{\zhongkai}[1]{\comment{brown}{Zhongkai}{#1}}

\renewcommand\floatpagefraction{.99}
\renewcommand\topfraction{.99}
\renewcommand\bottomfraction{.99}
\renewcommand\textfraction{.1}   
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}

\title{Towards explainable computer assisted Neuroanatomy}
\author{Kui Qian, Zhongkai Wu, Beth Friedman, David Kleinfeld, Yoav Freund}
\date{May 2022}

\begin{document}

\maketitle

\begin{abstract}
  A fundamental goal of neuroanatomy is the annotation of cells and
  structures.  Manual annotation of structures is based on the spatial
  distribution of cell shape, size, orientation and density.  New
  technology rapid imaging of entire brains at high resolution.
  However, manual identification of structures in these massive
  datasets is prohibitively time consuming.  We present a machine
  learning methodology for developing digital assistants that
  significantly reduce the labor of the anatomist while improving the
  consistency of the annotation.

  Our methodology is based on designing, for each annotation problem,
  a large number of features and using boosting on manually annotated
  images to select the most predictive features.

  We describe two annotation tasks to which we applied this approach.
  The first, more complex task is detecting specific structures in the
  brainstem of the mouse. The second is the detection of individual cells marked
  by a fluorescent dye. We provide the details of both of these
  applications and show that they significantly reduce the anatomist's
  labor in completing these tasks.
  
  %We translate each cell image into a feature vector that
  %includes aspect ratio, orientation and area, as well as additional
  %features derived using a graph Laplacian. The algorithm uses the
  %statistical distribution of these features vectors to identify brain
  %structures.
\end{abstract}

\section{Main}
\subsection{Summary}
We propose a system for detecting anatomical structures in the mouse
brain. Our system takes as input high-resolution images of aligned
sections. It produces as output the estimated center of mass (COM) for
each structure. Each detection is assigned a confidence. High
confidence structures are associated with a visual explanation.

\subsubsection {Significance for Neuroscience}
~\\ ~\\
{\bf David and Beth, please expand on the following bullets:}
\begin{itemize}
\item What is Neuroanatomy?
\item Why is Neuroanatomy important (in the context of the great
    advancements in imaging and recording technology)
\item Why is Neuroanatomy hard? (requires a lot of training, the
  number of trained neuroanatomist is small.)
\item An estimate of the amount of manual labor required to achieve
  common tasks.
\item The importance of computer generated confidence and explanations.
\end{itemize}

% Manual neuroanatomical analysis of brain sections is critical for
% creating accurate quantitative results. On the other hand, performing
% neuroanatomical analysis is expensive as it requires many hours of
% work of an expert anatomist.

We consider two labor intensive tasks. Localizing anatomical structure
and counting marked cells. Both tasks involve identifying and marking
locations in images and both require expertise.

A rather obvious but important observation is that while a typical
section will contain hard to identify locations, most of the locations
are easy, in the sense that a person with lesser traing can identify
them reliably.

This observation leads to the following approach to automation. We
use so-called {\em confidence rated detectors}. These detectors
associate with each detection a confidence score. A high confidence
score implies that the example is ``easy''  while the rest are
``hard''. This reduces the workload on the anatomist to the following
tasks:
\begin{itemize}
  \item {\bf Confirming the confident detections} In this step the
    anatomist recieves a small sample of the confident detections and
    verifies that they are correct. The sample size depends on the
    desired accuracy.
    \item {\bf classifiying the unconfident detections} In this step
      the anatomist labels {\em all} of the low confident predictions.
    \item {\bf searching for misses:} In this step the anatomist looks
      for locations that were completely missed by te detector.
  \end{itemize}
  
\subsubsection{Assistive AI} Traditionally, the goal of artificial
  intelligence is to design algorithms which mimic human behavior. AI
  systems are expected to perform as well as humans and, eventually,
  better than humans.

Indeed, in some domains, such as the game of go \cite{silver2017mastering} deep neural
networks (DNN)  have achieved super-human capabilities. In addition,
on highly curated image classification datasets such as ImageNet~\cite{} and
CIFAR~\cite{} DNNs give performance that, some argue, is better than
human. 

However, DNNs have some fundamental drawbacks that limit their
applicability to real-world problems. One problem is that DNNs
require large amounts of training data. In some
setups, such as alpha-go, the labels are generated automatically as
the program plays against itself. When the labels are generated by a
human, as is the case in neuroanatomy, collecting large and reliable
labels is prohibitavely expensive. A second
problem is that the training data and the test data are assumed to
come from the same distribution. However, in most real-world
applications the test data is drawn after the training is complete and
is often generated by a setup that is different from the one used to
collect the training data.
Specifically, in brain imaging, two brain section images from the same
location in different brains are likely to look quite differently.
These images vary for a variety of reasons including: animal to animal variability,
variability in the preparation process, including staining,
sectioning and imaging. In order to create robust classifiers, that do
not need to be retrained on each brain, we need a learning algorithm
that identifies robust patterns that are preserved across brains.
identifies patterns that are consistent across brains.

A third problem arises from variability in human labeling. We use
the term {\em inter rater variation} to refer to the difference in
labels between anatomists. We use the term {\em intra rater
  variation} to refer to the difference in the labels generated by the
same anatomist at different times. Inter and intra rater variations
have been studied in medicine~\cite{gellhorn2013inter}. One
contribution of this paper is an evaluation of rater variation in
neuroanatomy and it's relation to the {\em confident/un-confident}
classifications generated by our methods.


\subsubsection{Cell based approach}  Most CNN-based method of image
analysis are based on the concept of a sliding window. This implies
that an
y explanation provided by the CNN is based on the content of one or more windows. Windows need to be large enough to capture the image statistics. This means that a typical window contains 10-100 cells.
On the other hand, anatomist base their analysis on cytoarchitecture, which, in turn, is based on the shapes of individual cells and the relationshp between them. This makes it harder for the anatomist to understand the decisions of the detector.
\begin{wrapfigure}{R}{0.65\textwidth}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/Training.pdf}
  \caption{Training \label{fig:training}}
\end{wrapfigure}

To remedy this problem and make the detections explainable, we use individual cells as our basic unit. 

The overall design of the system is shown in Figure xxx. The system can be partitioned into two parts. The training algorithm and the trained detector.

\subsection{System Inputs}
To train the detectors we combine three sources of information:
\begin{itemize}
    \item {\bf Images of aligned sections:} The Nissl images of XX brains are the primary information source. They are also by far the largest at XXX GB per brain.
    \item {\bf The Atlas} is a representation of the shape of each structure and the relative locations of the structures. The atlas was constructed through a concensus ....
    \item {\bf COMs for individual brains:} For XX brains we had an anatomist locate the COM ....
\end{itemize}

\subsection{System components}

\begin{enumerate}
\item{\bf Cell extraction and shape parametrization}
An image of a single cell is typically around 50$\times$50 pixels, or a 2500 dimensional vector. The dimension of this representation is too high for effective machine learning. We therefor seek a dimensionality reduction mapping. This mapping consists of three parts:
\begin{enumerate}
    \item {\bf Normalization:} We normalize the image of the cell in three ways. We 
    shift the grey-levels so that the mean is XXX, scale the grey levels so that the standard deviation is YYY, and rotate the image so that the long axis of the cell is in angle zero. The three parameters associated with the normalizations define three features.
    \item {\bf Standard shape features}: we use the Hu features: x,y,z...
    \item{ \bf  Diffusion maps}: Diffusion Mapping~\cite{Belkin,
        Coifman} is a non-linear dimensionality reduction method that
      is based on a graphical representation of the data and on the
      Laplace operator on graphs.
      \end{enumerate}
\end{enumerate}
      \subsubsection{ Characterizing Cytoarchitecture} uses Difference between the CDFs for inside and outside of each structure (based on manually annotated structure boundaries). 
\subsubsection { Structure detectors} Combine the difference-of-CDF
features using boosted trees (XGBoost). Each structure has a
corresponding detector.

\subsubsection { Structure detection confidence} The confidence of structure
  detections is measured by the prediction margin and by the sharpness
  of the detection peak.
\begin{wrapfigure}{R}{0.65\textwidth}
\centering
\includegraphics[width=0.6\textwidth]{figures/Training.pdf}
\caption{Training \label{fig:training}}
\end{wrapfigure}
y
\subsubsection { Structure detection explanation}

\iffalse
which is based on cells as the basic unit. Doing so provides an explanation for the detector's decision. We use unsupervised learning to find a dimensionality reducing mapping for cell shapes. We make efficient use of manual annotations by estimating the shape of each structure from a few brains and anatomists. We leverage this information in the detector training by having the anatomist identify the location of the center of mass for each structure.
\fi
\section{Results}

\begin{itemize}
    \item Detection results, with confidence levels, consistency with Beth.
    \yoav{A lot of stuff here. Detection accuracy and confidence table, full table will probably go in the appendix, the figure of improvement. Relationship between beth, rough alignment and detection.}
    \item {\bf Explaining detections} The explanation of the detection is expressed in terms of cells whose shape gives evidence to the structure.
    \yoav{Put here an image consisting of 2/3 detection explanations to show the explanation power. Specifically, there is a structure we detect well even though in different brains the contrast and brightness are very different}
\item {\bf Cell shape parametrization} Uses a combination of Hu moments and dimensionality reduction using eigen-maps.
\begin{itemize}
    \item Eigen-maps learn a dimensionality reducing mapping cell shape to a ten dimensional representation.
    As it is an unsupervised method it requires no human labeling. We take advantage of the very large number of cells in single brain. 
    \item Each brain creates a different mapping, however, the mappings can be made consistent by adding a linear transformation. This creates a stable parametrization and makes the detections more consistent.\yoav{scatter-plots of cell images. after transformation. Figure explaining transformation.}
\end{itemize}

\end{itemize}

\section{Detecting marked cells}

William to put the {\em latest} table here. and write a detailed
explanation of the table.

To discuss:
\begin{itemize}
\item The amount of manual work that it takes to manually mark cells.
\item inter-rater and intra-rater disagreement rate.
\item How to count false detections "with approximately correct
  detections detection".
\item How to quantify reduced mental load of QA vs detection.
\end{itemize}


\section{Methods}
In this section we provide more details on the algorithms we use in
our system. The full code is available from github...

\begin{itemize}
\item Cell based features.
\item Diffusion mapping.
\item Calibration of the diffusion features.
\item Region features and CDFs.
\item Boosting and XGBoost.
\item Localizing structures. Rough alignment, computing detections
  scores, computing autocorrelation and asigning confidence.
\item Using the system for brain-to-atlas alignment.
\item Generating explanations.
\end{itemize}

\section{Unplaced figures}

\begin{figure}[t]
  \includegraphics[width=\textwidth]{figures/DetectionConfidenceVsError.png}
  \caption{}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\textwidth]{figures/DetectionsExplanations.png}
  \caption{}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\textwidth]{figures/MarkedCellsDetectionNumbers.png}
  \caption{}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\textwidth]{figures/Marked_cell_detections.png}
  \caption{}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\textwidth]{figures/RoughVSDEtection.png}
  \caption{}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\textwidth]{figures/ScoresOfStructures.png}
  \caption{}
\end{figure}

\end{document}


